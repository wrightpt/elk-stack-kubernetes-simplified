Directory Structure:
elk-stack-repo/
│   ├── cloudflare-secret.yaml
│   ├── default-traefik-values.yaml
│   ├── elasticsearch-git.yaml
│   ├── elk-ingress-routes.yaml
│   ├── kibana-git.yaml
│   ├── traefik-pv.yaml
│   ├── traefik-storage.yaml
│   ├── logstash-git.yaml
│   ├── traefik-git.yaml
argocd/
│   ├── setup/
│   ├── │   ├── argocd-installation.yaml
│   ├── applications/
│   ├── │   ├── logstash.yaml
│   ├── │   ├── metallb.yaml
│   ├── │   ├── elasticsearch.yaml
│   ├── │   ├── kibana.yaml
│   ├── │   ├── traefik.yaml
│   ├── │   ├── metallb-config.yaml
.git/
│   ├── logs/
│   │   ├── refs/
│   │   │   ├── heads/
│   │   │   ├── remotes/
│   │   │   │   ├── origin/
│   ├── hooks/
│   ├── info/
│   ├── objects/
│   │   ├── 7a/
│   │   ├── e1/
│   │   ├── ab/
│   │   ├── 25/
│   │   ├── 64/
│   │   ├── 3c/
│   │   ├── aa/
│   │   ├── 32/
│   │   ├── 8b/
│   │   ├── pack/
│   │   ├── b3/
│   │   ├── 47/
│   │   ├── ca/
│   │   ├── db/
│   │   ├── fe/
│   │   ├── 88/
│   │   ├── 79/
│   │   ├── 20/
│   │   ├── 17/
│   │   ├── 8d/
│   │   ├── 05/
│   │   ├── 49/
│   │   ├── 0a/
│   │   ├── 22/
│   │   ├── e2/
│   │   ├── 0e/
│   │   ├── 24/
│   │   ├── 6e/
│   │   ├── 8c/
│   │   ├── 2c/
│   │   ├── ef/
│   │   ├── 97/
│   │   ├── 53/
│   │   ├── 3b/
│   │   ├── d7/
│   │   ├── 03/
│   │   ├── 82/
│   │   ├── b8/
│   │   ├── 11/
│   │   ├── d6/
│   │   ├── f8/
│   │   ├── 42/
│   │   ├── 6c/
│   │   ├── 10/
│   │   ├── 39/
│   │   ├── 38/
│   │   ├── 14/
│   │   ├── 84/
│   │   ├── a7/
│   │   ├── 81/
│   │   ├── 75/
│   │   ├── 96/
│   │   ├── cd/
│   │   ├── 58/
│   │   ├── 91/
│   │   ├── 60/
│   │   ├── 34/
│   │   ├── f9/
│   │   ├── ac/
│   │   ├── 9f/
│   │   ├── b9/
│   │   ├── 93/
│   │   ├── 36/
│   │   ├── 0c/
│   │   ├── a1/
│   │   ├── 3a/
│   │   ├── e6/
│   │   ├── 94/
│   │   ├── 55/
│   │   ├── 16/
│   │   ├── 51/
│   │   ├── 78/
│   │   ├── d2/
│   │   ├── 5e/
│   │   ├── 92/
│   │   ├── b2/
│   │   ├── e4/
│   │   ├── 15/
│   │   ├── 09/
│   │   ├── 9a/
│   │   ├── d4/
│   │   ├── c7/
│   │   ├── 62/
│   │   ├── 66/
│   │   ├── a3/
│   │   ├── 5a/
│   │   ├── 90/
│   │   ├── d5/
│   │   ├── b0/
│   │   ├── f6/
│   │   ├── da/
│   │   ├── 01/
│   │   ├── fd/
│   │   ├── 07/
│   │   ├── 41/
│   │   ├── 3f/
│   │   ├── bf/
│   │   ├── 68/
│   │   ├── 27/
│   │   ├── c4/
│   │   ├── ed/
│   │   ├── 2a/
│   │   ├── e5/
│   │   ├── 3d/
│   │   ├── 35/
│   │   ├── ae/
│   │   ├── 4b/
│   │   ├── 98/
│   │   ├── bc/
│   │   ├── a2/
│   │   ├── 02/
│   │   ├── 8e/
│   │   ├── e0/
│   │   ├── 7c/
│   │   ├── c3/
│   │   ├── 50/
│   │   ├── f1/
│   │   ├── 9e/
│   │   ├── 13/
│   │   ├── bb/
│   │   ├── c8/
│   │   ├── ba/
│   │   ├── 08/
│   │   ├── 2b/
│   │   ├── e3/
│   │   ├── 4a/
│   │   ├── 30/
│   │   ├── d0/
│   │   ├── 87/
│   │   ├── de/
│   │   ├── af/
│   │   ├── be/
│   │   ├── 2e/
│   │   ├── 80/
│   │   ├── f7/
│   │   ├── 19/
│   │   ├── 37/
│   │   ├── ec/
│   │   ├── 12/
│   │   ├── e8/
│   │   ├── 71/
│   │   ├── info/
│   │   ├── ad/
│   │   ├── 8f/
│   │   ├── 04/
│   │   ├── 40/
│   │   ├── f3/
│   │   ├── 46/
│   │   ├── 7b/
│   │   ├── 1c/
│   │   ├── 86/
│   │   ├── 6d/
│   │   ├── 1e/
│   │   ├── f2/
│   │   ├── 31/
│   │   ├── 5b/
│   │   ├── c9/
│   │   ├── 7f/
│   │   ├── f0/
│   │   ├── c0/
│   │   ├── 65/
│   │   ├── 21/
│   │   ├── 99/
│   │   ├── 69/
│   │   ├── 5d/
│   │   ├── 54/
│   │   ├── e9/
│   │   ├── 63/
│   │   ├── 67/
│   │   ├── eb/
│   │   ├── 76/
│   │   ├── 56/
│   │   ├── a0/
│   │   ├── 06/
│   │   ├── 9d/
│   │   ├── ea/
│   │   ├── 29/
│   │   ├── c2/
│   │   ├── dd/
│   │   ├── ce/
│   │   ├── 0b/
│   │   ├── b6/
│   │   ├── b7/
│   │   ├── 74/
│   │   ├── 3e/
│   │   ├── cf/
│   │   ├── 6f/
│   │   ├── 4f/
│   ├── branches/
│   ├── refs/
│   │   ├── heads/
│   │   ├── tags/
│   │   ├── remotes/
│   │   │   ├── origin/
docker/
│   ├── elasticsearch/
│   ├── │   ├── Dockerfile
│   ├── traefik/
│   ├── │   ├── Dockerfile
│   ├── logstash/
│   ├── │   ├── Dockerfile
│   ├── kibana/
│   ├── │   ├── Dockerfile
helm/
│   ├── elasticsearch/
│   ├── │   ├── values.yaml
│   ├── │   ├── Chart.yaml
│   ├── traefik/
│   ├── │   ├── values.yaml
│   ├── │   ├── Chart.yaml
│   ├── logstash/
│   ├── │   ├── values.yaml
│   ├── │   ├── Chart.yaml
│   ├── kibana/
│   ├── │   ├── values.yaml
│   ├── │   ├── Chart.yaml

=== File: cloudflare-secret.yaml ===
apiVersion: v1
# REDACTED: Sensitive content removed
metadata:
  name: cloudflare-api-token
  namespace: traefik
type: Opaque
stringData:
  CF_API_TOKEN: "BLh7Yg7p3oJQ_U1k2T9zKjswLXTUgZSiud6ZYIXL"


=== File: default-traefik-values.yaml ===
#elk-stack-repo/default-traefik-values.yaml
# Default values for Traefik
image:
  name: traefik
  # defaults to appVersion
  tag: ""
  pullPolicy: IfNotPresent

#
# Configure the deployment
#
deployment:
  enabled: true
  # Can be either Deployment or DaemonSet
  kind: Deployment
  # Number of pods of the deployment (only applies when kind == Deployment)
  replicas: 1
  # Number of old history to retain to allow rollback (If not set, default Kubernetes value is set to 10)
  # revisionHistoryLimit: 1
  # Amount of time (in seconds) before Kubernetes will send the SIGKILL signal if Traefik does not shut down
  terminationGracePeriodSeconds: 60
  # The minimum number of seconds Traefik needs to be up and running before the DaemonSet/Deployment controller considers it available
  minReadySeconds: 0
  # Additional deployment annotations (e.g. for jaeger-operator sidecar injection)
  annotations: {}
  # Additional deployment labels (e.g. for filtering deployment by custom labels)
  labels: {}
  # Additional pod annotations (e.g. for mesh injection or prometheus scraping)
  podAnnotations: {}
  # Additional Pod labels (e.g. for filtering Pod by custom labels)
  podLabels: {}
  # Additional containers (e.g. for metric offloading sidecars)
  additionalContainers: []
    # https://docs.datadoghq.com/developers/dogstatsd/unix_socket/?tab=host
    # - name: socat-proxy
    # image: alpine/socat:1.0.5
    # args: ["-s", "-u", "udp-recv:8125", "unix-sendto:/socket/socket"]
    # volumeMounts:
    #   - name: dsdsocket
    #     mountPath: /socket
  # Additional volumes available for use with initContainers and additionalContainers
  additionalVolumes: []
    # - name: dsdsocket
    #   hostPath:
    #     path: /var/run/statsd-exporter
  # Additional initContainers (e.g. for setting file permission as shown below)
  initContainers: []
    # The "volume-permissions" init container is required if you run into permission issues.
    # Related issue: https://github.com/traefik/traefik/issues/6972
    # - name: volume-permissions
    #   image: busybox:1.31.1
    #   command: ["sh", "-c", "chmod -Rv 600 /data/*"]
    #   volumeMounts:
    #     - name: data
    #       mountPath: /data
  # Use process namespace sharing
  shareProcessNamespace: false
  # Custom pod DNS policy. Apply if `hostNetwork: true`
  # dnsPolicy: ClusterFirstWithHostNet
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed

# Pod disruption budget
podDisruptionBudget:
  enabled: false
  # maxUnavailable: 1
  # maxUnavailable: 33%
  # minAvailable: 0
  # minAvailable: 25%

# Use ingressClass. Ignored if Traefik version < 2.3 / kubernetes < 1.18.x
ingressClass:
  # true is not unit-testable yet, pending https://github.com/rancher/helm-unittest/pull/12
  enabled: false
  isDefaultClass: false
  # Use to force a networking.k8s.io API Version for certain CI/CD applications. E.g. "v1beta1"
  fallbackApiVersion: ""

# Activate Pilot integration
pilot:
  enabled: false
  token: ""
  # Toggle Pilot Dashboard
  # dashboard: false

# Enable experimental features
experimental:
  http3:
    enabled: false
  plugins:
    enabled: false
  kubernetesGateway:
    enabled: false
# REDACTED: Sensitive content removed
    #   group: "core"
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
    # By default, Gateway would be created to the Namespace you are deploying Traefik to.
    # You may create that Gateway in another namespace, setting its name below:
    # namespace: default

# Create an IngressRoute for the dashboard
ingressRoute:
  dashboard:
    enabled: true
    # Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class)
    annotations: {}
    # Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels)
    labels: {}

rollingUpdate:
  maxUnavailable: 1
  maxSurge: 1

# Customize liveness and readiness probe values.
readinessProbe:
  failureThreshold: 1
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 2

livenessProbe:
  failureThreshold: 3
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 2

#
# Configure providers
#
providers:
  kubernetesCRD:
    enabled: true
    allowCrossNamespace: false
    allowExternalNameServices: false
    allowEmptyServices: false
    # ingressClass: traefik-internal
    # labelSelector: environment=production,method=traefik
    namespaces: []
      # - "default"

  kubernetesIngress:
    enabled: true
    allowExternalNameServices: false
    allowEmptyServices: false
    # ingressClass: traefik-internal
    # labelSelector: environment=production,method=traefik
    namespaces: []
      # - "default"
    # IP used for Kubernetes Ingress endpoints
    publishedService:
      enabled: false
      # Published Kubernetes Service to copy status from. Format: namespace/servicename
      # By default this Traefik service
      # pathOverride: ""

#
# Add volumes to the traefik pod. The volume name will be passed to tpl.
# This can be used to mount a cert pair or a configmap that holds a config.toml file.
# After the volume has been mounted, add the configs into traefik by using the `additionalArguments` list below, eg:
# additionalArguments:
# - "--providers.file.filename=/config/dynamic.toml"
# - "--ping"
# - "--ping.entrypoint=web"
volumes: []
# - name: public-cert
#   mountPath: "/certs"
# REDACTED: Sensitive content removed
# - name: '{{ printf "%s-configs" .Release.Name }}'
#   mountPath: "/config"
#   type: configMap

# Additional volumeMounts to add to the Traefik container
additionalVolumeMounts: []
  # For instance when using a logshipper for access logs
  # - name: traefik-logs
  #   mountPath: /var/log/traefik

# Logs
# https://docs.traefik.io/observability/logs/
logs:
  # Traefik logs concern everything that happens to Traefik itself (startup, configuration, events, shutdown, and so on).
  general:
    # By default, the logs use a text format (common), but you can
    # also ask for the json format in the format option
    # format: json
    # By default, the level is set to ERROR. Alternative logging levels are DEBUG, PANIC, FATAL, ERROR, WARN, and INFO.
    level: ERROR
  access:
    # To enable access logs
    enabled: false
    # By default, logs are written using the Common Log Format (CLF).
    # To write logs in JSON, use json in the format option.
    # If the given format is unsupported, the default (CLF) is used instead.
    # format: json
    # To write the logs in an asynchronous fashion, specify a bufferingSize option.
    # This option represents the number of log lines Traefik will keep in memory before writing
    # them to the selected output. In some cases, this option can greatly help performances.
    # bufferingSize: 100
    # Filtering https://docs.traefik.io/observability/access-logs/#filtering
    filters: {}
      # statuscodes: "200,300-302"
      # retryattempts: true
      # minduration: 10ms
    # Fields
    # https://docs.traefik.io/observability/access-logs/#limiting-the-fieldsincluding-headers
    fields:
      general:
        defaultmode: keep
        names: {}
          # Examples:
          # ClientUsername: drop
      headers:
        defaultmode: drop
        names: {}
          # Examples:
          # User-Agent: redact
          # Authorization: drop
          # Content-Type: keep

metrics:
  # datadog:
  #   address: 127.0.0.1:8125
  # influxdb:
  #   address: localhost:8089
  #   protocol: udp
  prometheus:
    entryPoint: metrics
  #  addRoutersLabels: true
  # statsd:
  #   address: localhost:8125

tracing: {}
  # instana:
  #   enabled: true
  # datadog:
  #   localAgentHostPort: 127.0.0.1:8126
  #   debug: false
  #   globalTag: ""
  #   prioritySampling: false

globalArguments:
  - "--global.checknewversion"
  - "--global.sendanonymoususage"

#
# Configure Traefik static configuration
# Additional arguments to be passed at Traefik's binary
# All available options available on https://docs.traefik.io/reference/static-configuration/cli/
## Use curly braces to pass values: `helm install --set="additionalArguments={--providers.kubernetesingress.ingressclass=traefik-internal,--log.level=DEBUG}"`
additionalArguments: []
#  - "--providers.kubernetesingress.ingressclass=traefik-internal"
#  - "--log.level=DEBUG"

# Environment variables to be passed to Traefik's binary
env: []
# - name: SOME_VAR
#   value: some-var-value
# - name: SOME_VAR_FROM_CONFIG_MAP
#   valueFrom:
#     configMapRef:
#       name: configmap-name
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
#   valueFrom:
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed

envFrom: []
# - configMapRef:
#     name: config-map-name
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed

# Configure ports
ports:
  # The name of this one can't be changed as it is used for the readiness and
  # liveness probes, but you can adjust its config to your liking
  traefik:
    port: 9000
    # Use hostPort if set.
    # hostPort: 9000
    #
    # Use hostIP if set. If not set, Kubernetes will default to 0.0.0.0, which
    # means it's listening on all your interfaces and all your IPs. You may want
    # to set this value if you need traefik to listen on specific interface
    # only.
    # hostIP: 192.168.100.10

    # Override the liveness/readiness port. This is useful to integrate traefik
    # with an external Load Balancer that performs healthchecks.
    # healthchecksPort: 9000

    # Defines whether the port is exposed if service.type is LoadBalancer or
    # NodePort.
    #
    # You SHOULD NOT expose the traefik port on production deployments.
    # If you want to access it from outside of your cluster,
    # use `kubectl port-forward` or create a secure ingress
    expose: false
    # The exposed port for this service
    exposedPort: 9000
    # The port protocol (TCP/UDP)
    protocol: TCP
  web:
    port: 8000
    # hostPort: 8000
    expose: true
    exposedPort: 80
    # The port protocol (TCP/UDP)
    protocol: TCP
    # Use nodeport if set. This is useful if you have configured Traefik in a
    # LoadBalancer
    # nodePort: 32080
    # Port Redirections
    # Added in 2.2, you can make permanent redirects via entrypoints.
    # https://docs.traefik.io/routing/entrypoints/#redirection
    # redirectTo: websecure
  websecure:
    port: 8443
    # hostPort: 8443
    expose: true
    exposedPort: 443
    # The port protocol (TCP/UDP)
    protocol: TCP
    # nodePort: 32443
    # Enable HTTP/3.
    # Requires enabling experimental http3 feature and tls.
    # Note that you cannot have a UDP entrypoint with the same port.
    # http3: true
    # Set TLS at the entrypoint
    # https://doc.traefik.io/traefik/routing/entrypoints/#tls
    tls:
      enabled: false
      # this is the name of a TLSOption definition
      options: ""
      certResolver: ""
      domains: []
      # - main: example.com
      #   sans:
      #     - foo.example.com
      #     - bar.example.com
  metrics:
    port: 9100
    # hostPort: 9100
    # Defines whether the port is exposed if service.type is LoadBalancer or
    # NodePort.
    #
    # You may not want to expose the metrics port on production deployments.
    # If you want to access it from outside of your cluster,
    # use `kubectl port-forward` or create a secure ingress
    expose: false
    # The exposed port for this service
    exposedPort: 9100
    # The port protocol (TCP/UDP)
    protocol: TCP

# TLS Options are created as TLSOption CRDs
# https://doc.traefik.io/traefik/https/tls/#tls-options
# Example:
# tlsOptions:
#   default:
#     sniStrict: true
#     preferServerCipherSuites: true
#   foobar:
#     curvePreferences:
#       - CurveP521
#       - CurveP384
tlsOptions: {}

# Options for the main traefik service, where the entrypoints traffic comes
# from.
service:
  enabled: true
  type: LoadBalancer
  # Additional annotations applied to both TCP and UDP services (e.g. for cloud provider specific config)
  annotations: {}
  # Additional annotations for TCP service only
  annotationsTCP: {}
  # Additional annotations for UDP service only
  annotationsUDP: {}
  # Additional service labels (e.g. for filtering Service by custom labels)
  labels: {}
  # Additional entries here will be added to the service spec.
  # Cannot contain type, selector or ports entries.
  spec: {}
    # externalTrafficPolicy: Cluster
    # loadBalancerIP: "1.2.3.4"
    # clusterIP: "2.3.4.5"
  loadBalancerSourceRanges: []
    # - 192.168.0.1/32
    # - 172.16.0.0/16
  externalIPs: []
    # - 1.2.3.4
  # One of SingleStack, PreferDualStack, or RequireDualStack.
  # ipFamilyPolicy: SingleStack
  # List of IP families (e.g. IPv4 and/or IPv6).
  # ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
  # ipFamilies:
  #   - IPv4
  #   - IPv6

## Create HorizontalPodAutoscaler object.
##
autoscaling:
  enabled: false
#   minReplicas: 1
#   maxReplicas: 10
#   metrics:
#   - type: Resource
#     resource:
#       name: cpu
#       targetAverageUtilization: 60
#   - type: Resource
#     resource:
#       name: memory
#       targetAverageUtilization: 60

# Enable persistence using Persistent Volume Claims
# ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
# After the pvc has been mounted, add the configs into traefik by using the `additionalArguments` list below, eg:
# additionalArguments:
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
persistence:
  enabled: false
  name: data
#  existingClaim: ""
  accessMode: ReadWriteOnce
  size: 128Mi
  # storageClass: ""
  path: /data
  annotations: {}
  # subPath: "" # only mount a subpath of the Volume into the pod

certResolvers: {}
#   letsencrypt:
#     # for challenge options cf. https://doc.traefik.io/traefik/https/acme/
#     email: email@example.com
#     dnsChallenge:
#       # also add the provider's required configuration under env
# REDACTED: Sensitive content removed
#       # cf. https://doc.traefik.io/traefik/https/acme/#providers
#       provider: digitalocean
#       # add futher options for the dns challenge as needed
#       # cf. https://doc.traefik.io/traefik/https/acme/#dnschallenge
#       delayBeforeCheck: 30
#       resolvers:
#         - 1.1.1.1
#         - 8.8.8.8
#     tlsChallenge: true
#     httpChallenge:
#       entryPoint: "web"
#     # match the path to persistence
#     storage: /data/acme.json

# If hostNetwork is true, runs traefik in the host network namespace
# To prevent unschedulabel pods due to port collisions, if hostNetwork=true
# and replicas>1, a pod anti-affinity is recommended and will be set if the
# affinity is left as default.
hostNetwork: false

# Whether Role Based Access Control objects like roles and rolebindings should be created
rbac:
  enabled: true

  # If set to false, installs ClusterRole and ClusterRoleBinding so Traefik can be used across namespaces.
  # If set to true, installs namespace-specific Role and RoleBinding and requires provider configuration be set to that same namespace
  namespaced: false

# Enable to create a PodSecurityPolicy and assign it to the Service Account via RoleBinding or ClusterRoleBinding
podSecurityPolicy:
  enabled: false

# The service account the pods will use to interact with the Kubernetes API
serviceAccount:
  # If set, an existing service account is used
  # If not set, a service account is created automatically using the fullname template
  name: ""

# Additional serviceAccount annotations (e.g. for oidc authentication)
serviceAccountAnnotations: {}

resources: {}
  # requests:
  #   cpu: "100m"
  #   memory: "50Mi"
  # limits:
  #   cpu: "300m"
  #   memory: "150Mi"
affinity: {}
# # This example pod anti-affinity forces the scheduler to put traefik pods
# # on nodes where no other traefik pods are scheduled.
# # It should be used when hostNetwork: true to prevent port conflicts
#   podAntiAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       - labelSelector:
#           matchExpressions:
# REDACTED: Sensitive content removed
#               operator: In
#               values:
#                 - {{ template "traefik.name" . }}
# REDACTED: Sensitive content removed
nodeSelector: {}
tolerations: []

# Pods can have priority.
# Priority indicates the importance of a Pod relative to other Pods.
priorityClassName: ""

# Set the container security context
# To run the container with ports below 1024 this will need to be adjust to run as root
securityContext:
  capabilities:
    drop: [ALL]
  readOnlyRootFilesystem: true
  runAsGroup: 65532
  runAsNonRoot: true
  runAsUser: 65532

podSecurityContext:
  fsGroup: 65532



=== File: elasticsearch-git.yaml ===
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: elasticsearch
  namespace: argocd
spec:
  project: default
  source:
    repoURL: git@github.com:wrightpt/elk-stack-kubernetes-simplified.git
    targetRevision: main
    path: helm
    helm:
      valueFiles:
        - elasticsearch-values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: elk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


=== File: elk-ingress-routes.yaml ===
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: elasticsearch-route
  namespace: traefik
spec:
  entryPoints:
    - websecure
  routes:
    - match: Host(`elasticsearch.1xmr.com`)
      kind: Rule
      services:
        - name: elasticsearch-master
          namespace: elk
          port: 9200
  tls:
    certResolver: letsencrypt

---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: kibana-route
  namespace: traefik
spec:
  entryPoints:
    - websecure
  routes:
    - match: Host(`kibana.1xmr.com`)
      kind: Rule
      services:
        - name: kibana
          namespace: elk
          port: 5601
  tls:
    certResolver: letsencrypt

---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: logstash-route
  namespace: traefik
spec:
  entryPoints:
    - websecure
  routes:
    - match: Host(`logstash.1xmr.com`)
      kind: Rule
      services:
        - name: logstash
          namespace: elk
          port: 5044
  tls:
    certResolver: letsencrypt


=== File: kibana-git.yaml ===
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kibana
  namespace: argocd
spec:
  project: default
  source:
    repoURL: git@github.com:wrightpt/elk-stack-kubernetes-simplified.git
    targetRevision: main
    path: helm
    helm:
      valueFiles:
        - kibana-values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: elk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


=== File: traefik-pv.yaml ===
#elk-stack-repo/traefik-pv
apiVersion: v1
kind: PersistentVolume
metadata:
  name: traefik-certs-pv
spec:
  capacity:
    storage: 128Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  local:
    path: /mnt/kube-volumes/traefik-certs  # CORRECT PATH: /mnt/kube-volumes/traefik-certs
  nodeAffinity:  # <-- RE-INTRODUCING nodeAffinity - IT IS REQUIRED
    required:
      nodeSelectorTerms:
        - matchExpressions:
# REDACTED: Sensitive content removed
              operator: In
              values:
                - daemon  # <-- Ensure 'daemon' is the correct node name from 'kubectl get nodes' output


=== File: traefik-storage.yaml ===
# traefik-storage.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: traefik-pv
spec:
  capacity:
    storage: 128Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/kube-volumes/traefik
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
# REDACTED: Sensitive content removed
          operator: In
          values:
          - daemon
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: traefik-data
  namespace: traefik
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 128Mi
  storageClassName: local-storage


=== File: logstash-git.yaml ===
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: logstash
  namespace: argocd
spec:
  project: default
  source:
    repoURL: git@github.com:wrightpt/elk-stack-kubernetes-simplified.git
    targetRevision: main
    path: helm
    helm:
      valueFiles:
        - logstash-values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: elk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


=== File: traefik-git.yaml ===
#/elk-stack-repo/traefik-git
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: traefik
  namespace: argocd
spec:
  project: default
  source:
    repoURL: "git@github.com:wrightpt/elk-stack-kubernetes-simplified.git"  # Your repo URL
    targetRevision: "main"
    path: helm/traefik
  destination:
    server: "https://kubernetes.default.svc"
    namespace: "traefik"
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true


=== File: argocd/setup/argocd-installation.yaml ===
# argocd/setup/argocd-installation.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: argocd
---
# This includes the core ArgoCD installation
# When applying, use: kubectl apply -f argocd-installation.yaml -n argocd


=== File: argocd/applications/logstash.yaml ===
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: logstash
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://helm.elastic.co
    chart: logstash
    targetRevision: 7.17.3
    helm:
      valueFiles:
        - values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: elk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


=== File: argocd/applications/metallb.yaml ===
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: metallb
  namespace: argocd
spec:
  project: default
  source:
    repoURL: 'git@github.com:wrightpt/elk-stack-kubernetes-simplified.git'
    targetRevision: main
    path: argocd/applications   # Adjust as needed
    directory:
      recurse: true             # If needed for scanning nested files
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: metallb-system
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true



=== File: argocd/applications/elasticsearch.yaml ===
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: elasticsearch
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://helm.elastic.co
    chart: elasticsearch
    targetRevision: 7.17.3
    helm:
      valueFiles:
        - values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: elk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


=== File: argocd/applications/kibana.yaml ===
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kibana
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://helm.elastic.co
    chart: kibana
    targetRevision: 7.17.3
    helm:
      valueFiles:
        - values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: elk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


=== File: argocd/applications/traefik.yaml ===
# file: argocd/applications/traefik.yaml
# file: argocd/applications/traefik.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: traefik
  namespace: argocd
spec:
  project: default
  source:
    repoURL: "git@github.com:wrightpt/elk-stack-kubernetes-simplified.git"
    targetRevision: "main"
    path: helm/traefik
  destination:
    server: "https://kubernetes.default.svc"
    namespace: "traefik"
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true


=== File: argocd/applications/metallb-config.yaml ===
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: home-lan-pool
  namespace: metallb-system
spec:
  addresses:
    - 192.168.1.240-192.168.1.250
  # Set autoAssign to 'true' if you want MetalLB to automatically assign IPs 
  # from this pool to LoadBalancer Services.
  autoAssign: true

---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: home-lan-advert
  namespace: metallb-system
spec:
  # Name of the IPAddressPool to advertise on L2
  ipAddressPools:
    - home-lan-pool



=== File: docker/elasticsearch/Dockerfile ===
FROM docker.elastic.co/elasticsearch/elasticsearch:7.17.3

# Copy custom configuration files
COPY elasticsearch.yml /usr/share/elasticsearch/config/
COPY jvm.options /usr/share/elasticsearch/config/

# Set permissions
USER root
RUN chmod 660 /usr/share/elasticsearch/config/elasticsearch.yml \
    && chmod 660 /usr/share/elasticsearch/config/jvm.options \
    && chown -R 1000:0 /usr/share/elasticsearch/config

# Switch back to elasticsearch user
USER 1000


=== File: docker/traefik/Dockerfile ===
FROM traefik:v2.9

# Copy Traefik configuration
COPY traefik.yml /etc/traefik/
COPY config.yml /etc/traefik/

# REDACTED: Sensitive content removed
RUN mkdir -p /letsencrypt


=== File: docker/logstash/Dockerfile ===
FROM docker.elastic.co/logstash/logstash:7.17.3

# Copy configuration files
COPY logstash.yml /usr/share/logstash/config/
COPY pipelines.yml /usr/share/logstash/config/
COPY jvm.options /usr/share/logstash/config/
COPY conf.d/ /usr/share/logstash/pipeline/

# Set permissions
USER root
RUN chmod 660 /usr/share/logstash/config/logstash.yml \
    && chmod 660 /usr/share/logstash/config/pipelines.yml \
    && chmod 660 /usr/share/logstash/config/jvm.options \
    && chmod -R 660 /usr/share/logstash/pipeline/ \
    && chown -R 1000:0 /usr/share/logstash/config \
    && chown -R 1000:0 /usr/share/logstash/pipeline

# Switch back to logstash user
USER 1000


=== File: docker/kibana/Dockerfile ===
FROM docker.elastic.co/kibana/kibana:7.17.3

# Copy custom configuration
COPY kibana.yml /usr/share/kibana/config/

# Set permissions
USER root
RUN chmod 660 /usr/share/kibana/config/kibana.yml \
    && chown -R 1000:0 /usr/share/kibana/config

# Switch back to kibana user
USER 1000


=== File: helm/elasticsearch/values.yaml ===
# helm/elasticsearch-values.yaml
replicas: 1  # Adjust based on your home server capacity

# Resource requests and limits
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "1"
    memory: "2Gi"

# Persistence configuration
persistence:
  enabled: true
  storageClass: "local-storage"
  size: "30Gi"

# Network settings
service:
  type: ClusterIP

# Security settings
securityContext:
  enabled: true
  runAsUser: 1000
  fsGroup: 1000

# Config for ES
esConfig:
  elasticsearch.yml: |
    cluster.name: home-cluster
    discovery.type: single-node
    xpack.security.enabled: true
    xpack.security.transport.ssl.enabled: true


=== File: helm/elasticsearch/Chart.yaml ===
apiVersion: v2
name: elasticsearch
description: Elasticsearch Helm chart for Kubernetes
type: application
version: 1.0.0
appVersion: 7.17.3
dependencies:
  - name: elasticsearch
    version: 7.17.3
    repository: https://helm.elastic.co


=== File: helm/traefik/values.yaml ===
  # Modified values.yaml for helm/traefik
  # Service config
traefik:
  service:
    type: LoadBalancer
    externalIPs:
      - 192.168.1.240
    ports:
      web:
        port: 80
        protocol: TCP
        nodePort: 30080
      websecure:
        port: 443
        protocol: TCP
        nodePort: 30443
  # Traefik Dashboard
  dashboard:
    enabled: true
  # IngressRoute for the dashboard
  ingressRoute:
    dashboard:
      enabled: true
      rule: "Host(`traefik.local`) && PathPrefix(`/dashboard`)"
      tls:
        certResolver: letsencrypt
  providers:
    kubernetesCRD:
      enabled: true
    kubernetesIngress:
      enabled: true
  additionalArguments:
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
    - "--serverstransport.insecureskipverify=true"
    - "--log.level=DEBUG"
  persistence:
    enabled: true
    existingClaim: "traefik-data"  # Use the existing PVC
    path: /data
  env:
    - name: CF_API_TOKEN
      valueFrom:
# REDACTED: Sensitive content removed
          name: cloudflare-api-token
# REDACTED: Sensitive content removed
  # Rest of your configuration remains the same

  # Additional resources for dynamic config
  additionalResources:
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: traefik-config
        namespace: traefik
      data:
        config.yml: |
          http:
            routers:
              kibana:
                rule: "Host(`kibana.1xmr.com`)"
                entryPoints:
                  - websecure
                service: kibana
                tls:
                  certResolver: letsencrypt
              elasticsearch:
                rule: "Host(`elasticsearch.1xmr.com`)"
                entryPoints:
                  - websecure
                service: elasticsearch
                tls:
                  certResolver: letsencrypt
              logstash:
                rule: "Host(`logstash.1xmr.com`)"
                entryPoints:
                  - websecure
                service: logstash
                tls:
                  certResolver: letsencrypt

            services:
              kibana:
                loadBalancer:
                  servers:
                    - url: "http://kibana-kibana.elk:5601"
                passHostHeader: true
              elasticsearch:
                loadBalancer:
                  servers:
                    - url: "http://elasticsearch-master.elk:9200"
                passHostHeader: true
              logstash:
                loadBalancer:
                  servers:
                    - url: "http://logstash.elk:5044"
                passHostHeader: true


=== File: helm/traefik/Chart.yaml ===
#/elk-stack-repo/helm/traefik/Chart.yaml
apiVersion: v2
name: traefik
description: Traefik Helm chart for Kubernetes
type: application
version: 1.0.0
appVersion: 10.24.0
dependencies:
  - name: traefik
    version: 10.24.0
    repository: https://helm.traefik.io/traefik


=== File: helm/logstash/values.yaml ===
# helm/logstash-values.yaml
image:
  repository: "logstash"
  tag: "7.17.3"

# Resource requests and limits
resources:
  requests:
    cpu: "500m"
    memory: "2Gi"
  limits:
    cpu: "2"
    memory: "4Gi"

# Persistence configuration
persistence:
  enabled: true
  storageClass: "local-storage"
  size: "10Gi"

# Network settings
service:
  type: ClusterIP
  ports:
    - name: http
      port: 5044
      protocol: TCP

# Security settings
securityContext:
  runAsUser: 1000
  fsGroup: 1000

# Logstash configuration
logstashConfig:
  logstash.yml: |
    path.data: /usr/share/logstash/data
    path.logs: /var/log/logstash
    node.name: nextjs-market-logstash
    pipeline.workers: 8
    pipeline.batch.size: 1000
    pipeline.batch.delay: 50
    queue.type: persisted
    queue.max_bytes: 3gb
    log.level: debug

  pipelines.yml: |
    - pipeline.id: main
      path.config: "/usr/share/logstash/pipeline/*.conf"

# JVM options
logstashJavaOpts: "-Xms2g -Xmx2g"

# Pipeline configuration
logstashPipeline:
  nextjs-market.conf: |
    input {
      http {
        host => "0.0.0.0"
        port => 5044
        codec => json
        
        response_headers => {
          "Content-Type" => "application/json"
          "Access-Control-Allow-Origin" => "*"
          "Access-Control-Allow-Methods" => "POST, OPTIONS"
          "Access-Control-Allow-Headers" => "Content-Type"
        }
      }
    }

    filter {
      mutate {
        add_field => {
          "received_at" => "%{@timestamp}"
          "environment" => "${ENVIRONMENT}"
          "service" => "nextjs-market"
          "log_type" => "application"
        }
# REDACTED: Sensitive content removed
      }
    }

    output {
      elasticsearch {
        hosts => ["${ES_HOSTS}"]
        user => "${ES_USER}"
# REDACTED: Sensitive content removed
        index => "nextjs-market-${ENVIRONMENT}-%{+YYYY.MM.dd}"
        ssl_verification_mode => "none"
        timeout => 120
        retry_on_conflict => 5
        retry_max_interval => 30
        retry_initial_interval => 2
      }
      
      file {
        path => "/var/log/logstash/nextjs-market-debug.log"
        codec => json_lines
        flush_interval => 60
      }
    }

# Environment variables
extraEnvs:
  - name: ES_HOSTS
    value: "https://elasticsearch-master:9200"
  - name: ES_USER
    valueFrom:
# REDACTED: Sensitive content removed
        name: logstash-credentials
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
    valueFrom:
# REDACTED: Sensitive content removed
        name: logstash-credentials
# REDACTED: Sensitive content removed
  - name: ENVIRONMENT
    value: "production"


=== File: helm/logstash/Chart.yaml ===
apiVersion: v2
name: logstash
description: Logstash Helm chart for Kubernetes
type: application
version: 1.0.0
appVersion: 7.17.3
dependencies:
  - name: logstash
    version: 7.17.3
    repository: https://helm.elastic.co


=== File: helm/kibana/values.yaml ===
# helm/kibana-values.yaml
image:
  repository: "kibana"
  tag: "7.17.3"

elasticsearchHosts: "http://elasticsearch-master:9200"

# Resource requests and limits
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "1"
    memory: "2Gi"

# Persistence configuration
persistence:
  enabled: true
  storageClass: "local-storage"
  size: "5Gi"

# Network settings
service:
  type: ClusterIP
  port: 5601

# Security settings
securityContext:
  enabled: true
  runAsUser: 1000
  fsGroup: 1000

# Kibana configuration
kibanaConfig:
  kibana.yml: |
    server.host: "0.0.0.0"
    server.publicBaseUrl: "https://kibana.1xmr.com"
    elasticsearch.username: "kibana_admin"
# REDACTED: Sensitive content removed
    elasticsearch.ssl.verificationMode: none
# REDACTED: Sensitive content removed
    logging:
      appenders:
        file:
          type: file
          fileName: /var/log/kibana/kibana.log
          layout:
            type: json
      root:
        appenders:
          - default
          - file
    pid.file: /run/kibana/kibana.pid

# REDACTED: Sensitive content removed
extraEnvs:
# REDACTED: Sensitive content removed
    valueFrom:
# REDACTED: Sensitive content removed
        name: kibana-credentials
# REDACTED: Sensitive content removed
# REDACTED: Sensitive content removed
    valueFrom:
# REDACTED: Sensitive content removed
        name: kibana-credentials
# REDACTED: Sensitive content removed


=== File: helm/kibana/Chart.yaml ===
apiVersion: v2
name: kibana
description: Kibana Helm chart for Kubernetes
type: application
version: 1.0.0
appVersion: 7.17.3
dependencies:
  - name: kibana
    version: 7.17.3
    repository: https://helm.elastic.co


=== File: .github/workflows/build-images.yml ===


